hdfs dfs -rm -r /user/s1240267/data/output/exc-cw2/s1240267_task_x.out
hdfs dfs -cat /user/s1240267/data/output/exc-cw2/s1240267_task_4_2.out/part-00000 | head -20


Task 1

# TODO: sort files before printing, total ordering sort,

>>>>DOnt use 1 reducer
>>>> write another mapreduce job with cat as mapper and reducer to do the final sorting.

### Hadoop command ###:
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -input /data/assignments/ex2/task1/large/ \
 -output /user/s1240267/data/output/exc-cw2/s1240267_task_1.out \
 -numReduceTasks 1 \
 -mapper t1-map.py \
 -file t1-map.py \
 -combiner t1-combiner.py \
 -file t1-combiner.py \
 -reducer t1-reduce.py \
 -file t1-reduce.py




Task 2
>>>>>>>>> todo, words that dont appear in the corpus should get value of 0 (electronic in terms.txt), currently gets ignored
### Hadoop command ###:
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -input /user/s1240267/data/output/exc-cw2/s1240267_task_1.out \
 -output /user/s1240267/data/output/exc-cw2/s1240267_task_2.out \
 -numReduceTasks 1 \
 -mapper t2-map.py \
 -file t2-map.py \
 -reducer t2-reduce.py \
 -file t2-reduce.py \
 -file terms.txt

Task3_1
>>>>>> TODO write a combiner, reducer cant be used because it only outputs the max
>>>>>> increase number of reducers, write linux command to get the max

### Hadoop command ###:
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -input /data/assignments/ex2/task2/logsLarge.txt \
 -output /user/s1240267/data/output/exc-cw2/s1240267_task_3_1.out \
 -numReduceTasks 1 \
 -mapper t3_1_map.py \
 -file t3_1_map.py \
 -reducer t3_1_reduce.py \
 -file t3_1_reduce.py \
 -combiner t3_1_combine.py \
 -file t3_1_combine.py \
 -numReduceTasks 5


Task3_2
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -input /data/assignments/ex2/task2/logsLarge.txt \
 -output /user/s1240267/data/output/exc-cw2/s1240267_task_3_2.out \
 -numReduceTasks 5 \
 -mapper t3_2_map.py \
 -file t3_2_map.py \
 -reducer t3_2_reduce.py \
 -file t3_2_reduce.py \
 -combiner t3_2_combine.py \
 -file t3_2_combine.py

hdfs dfs -cat /user/s1240267/data/output/exc-cw2/s1240267_task_3_2.out/part-* | sort -rn | head -10 | hdfs dfs -put - /user/s1240267/data/output/exc-cw2/s1240267_task_3_2.out/top10.txt



Task3_3
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -input /data/assignments/ex2/task2/logsLarge.txt \
 -output /user/s1240267/data/output/exc-cw2/s1240267_task_3_3.out \
 -numReduceTasks 5 \
 -mapper t3_3_map.py \
 -file t3_3_map.py \
 -reducer t3_3_reduce.py \
 -file t3_3_reduce.py



Task4_1
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -input /data/assignments/ex2/task3/stackLarge.txt  \
 -output /user/s1240267/data/output/exc-cw2/s1240267_task_4_1.out \
 -numReduceTasks 10 \
 -mapper t4_1_map.py \
 -file t4_1_map.py \
 -reducer t4_1_reduce.py \
 -file t4_1_reduce.py

hdfs dfs -cat /user/s1240267/data/output/exc-cw2/s1240267_task_4_1.out/part-* | sort -rn -k 2| head -10 | hdfs dfs -put - /user/s1240267/data/output/exc-cw2/s1240267_task_4_1.out/top10.txt


Task4_2
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -input /data/assignments/ex2/task3/stackLarge.txt  \
 -output /user/s1240267/data/output/exc-cw2/s1240267_task_4_2.out \
 -numReduceTasks 10 \
 -mapper t4_2_map.py \
 -file t4_2_map.py \
 -reducer t4_2_reduce.py \
 -file t4_2_reduce.py

hdfs dfs -cat /user/s1240267/data/output/exc-cw2/s1240267_task_4_1.out/part-* | sort -rn -k 2| head -10 | hdfs dfs -put - /user/s1240267/data/output/exc-cw2/s1240267_task_4_1.out/top10.txt




 -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
